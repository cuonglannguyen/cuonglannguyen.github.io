---
title: "DPaI: Differentiable Pruning at Initialization with Node-Path Balance Principle"
collection: publications
category: conferences
permalink: /publication/2025-01-22-ICLR-DPAI
excerpt: 'Our paper proposed a differentiable pruning at initialization methods that achived significantly better performance on pruning at initialization tasks.'
date: 2025-01-22
venue: 'International Conference on Learning Representations (ICLR)'
paperurl: 'https://openreview.net/pdf?id=hvLBTpiDt3'
---

##Abstract##

Pruning at Initialization (PaI) is a technique in neural network optimization characterized by the proactive elimination of weights before the network's training on designated tasks. This innovative strategy potentially reduces the costs for training and inference, significantly advancing computational efficiency. A key factor leading to PaI's effectiveness is that it considers the saliency of weights in an untrained network, and prioritizes the trainability and optimization potential of the pruned subnetworks. Recent methods can effectively prevent the formation of hard-to-optimize networks, e.g. through iterative adjustments at each network layer. However, this way often results in large-scale discrete optimization problems, which could make PaI further challenging. This paper introduces a novel method, called DPaI, that involves a differentiable optimization of the pruning mask. DPaI adopts a dynamic and adaptable pruning process, allowing easier optimization processes and better solutions. More importantly, our differentiable formulation enables readily use of the existing rich body of efficient gradient-based methods for PaI. Our empirical results demonstrate that DPaI significantly outperforms current state-of-the-art PaI methods on various architectures, such as Convolutional Neural Networks and Vision-Transformers. Code is available at https://github.com/QuanNguyen-Tri/DPaI.git